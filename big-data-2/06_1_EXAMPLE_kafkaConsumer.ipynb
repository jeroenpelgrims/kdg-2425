{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-04T13:41:45.342102Z",
     "start_time": "2023-10-04T13:41:24.803755Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from delta import configure_spark_with_delta_pip\n",
    "from pyspark.sql import SparkSession\n",
    "import ConnectionConfig as cc\n",
    "cc.setupEnvironment()\n",
    "spark = cc.startLocalCluster(\"Kafka\")\n",
    "spark.getActiveSession()\n",
    "wordcount_delta_path = \"./spark-warehouse/kafkaWordCount\"\n",
    "#spark.sparkContext.setLogLevel(\"DEBUG\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Kafka\n",
    "In this cell reading from a Kafka topic is initiated.\n",
    "The enabled code gets the data from a docker container running Kafka.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-04T13:42:05.837311Z",
     "start_time": "2023-10-04T13:42:03.471021Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = (\n",
    "     spark.readStream\n",
    "     .format(\"kafka\")\n",
    "     .option(\"kafka.bootstrap.servers\", \"127.0.0.1:9092\")\n",
    "     .option(\"subscribe\", \"demo\")\n",
    "     #.option(\"startingOffsets\", \"earliest\")\n",
    "     .load()\n",
    " )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Converting the data to a dataframe\n",
    "json_tuple is an sql function that converts a json string to dataframe columns. The columns are named c0, c1, c2.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-04T13:42:11.732490Z",
     "start_time": "2023-10-04T13:42:11.485498Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "df  = df.selectExpr(\"json_tuple(CAST(value AS STRING), 'event_time', 'word', 'count')\")\n",
    "df.writeStream.format(\"console\").outputMode(\"update\").start(\"kafkaTest\")\n",
    "\n",
    "#df.writeStream.format(\"console\").outputMode(\"update\").option(\"checkpointLocation\",\".\\checkpoints\").start(\"kafkaTest\")\n",
    "# inter = df.selectExpr(\"json_tuple(CAST(value as STRING), 'timestamp', 'key', 'value')\")\n",
    "# inter.printSchema()\n",
    "#stream = inter.writeStream.format(\"console\").outputMode(\"update\").option(\"checkpointLocation\",\".\\checkpoints\").start(\"kafkaTest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "A string is converted to a timestamp and column names are changed to eventtime, key and value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-04T13:42:49.204770Z",
     "start_time": "2023-10-04T13:42:47.687002Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wordCountEvents= inter.selectExpr(\"cast(c0 as timestamp) as eventtime\" , \"c1 as key\", \"INT(c2) as value\").withWatermark(\"eventtime\",'10 seconds')\n",
    "wordCountEvents.printSchema()\n",
    "stream = wordCountEvents.writeStream.format(\"console\").outputMode(\"update\").option(\"checkpointLocation\",\".\\checkpoints1\").start(\"kafkaTest\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Wordcount aggregation\n",
    "The data is aggregated by a 10 second window and the key. The result is written to a delta table.\n",
    "The results are written to a delatable in append mode. This means that the data is added to the table after the window is closed after the watermark is passed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-04T13:43:14.765620Z",
     "start_time": "2023-10-04T13:43:13.370138Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "groupedEvents =wordCountEvents.groupBy(F.window(\"eventtime\", \"10 seconds\", \"10 seconds\"), \"key\").sum(\"value\").withColumnRenamed(\"sum(value)\", \"count\")\n",
    "groupedEvents.printSchema()\n",
    "stream = groupedEvents.writeStream.format(\"delta\").outputMode(\"append\").option(\"checkpointLocation\",\".\\checkpoints\").option(\"path\", wordcount_delta_path).start()\n",
    "#stream = groupedEvents.writeStream.format(\"console\").outputMode(\"update\").option(\"checkpointLocation\",\".\\checkpoints2\").start()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-04T11:48:20.772827Z",
     "start_time": "2023-10-04T11:48:20.746797Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stream.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-04T08:57:11.691142Z",
     "start_time": "2023-10-04T08:57:10.910147Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
